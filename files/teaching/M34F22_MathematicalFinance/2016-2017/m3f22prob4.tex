\documentclass[12pt]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{natbib}
\usepackage{color}
\def\bibfont{\footnotesize}
\setlength{\bibsep}{0pt plus 0.3ex}

\usepackage{url}

\begin{document}
\def\R{\mathbb{R}}
\def\C{\mathbb{C}}
\def\Z{\mathbb{Z}}
\def\N{\mathbb{N}}
\def\Q{\mathbb{Q}}
\def\D{\mathbb{D}}
\def\Sp{{\mathbb{S}}}
\def\T{\mathbb{T}}
\def\H{\mathbb{H}}
\def\hb{\hfil \break}
\def\ni{\noindent}
\def\i{\indent}
\def\a{\alpha}
\def\b{\beta}
\def\e{\epsilon}
\def\d{\delta}
\def\D{\Delta}
\def\G{\Gamma}
\def\g{\gamma}
\def\l{\lambda}
\def\m{\mu}
\def\s{\sigma}
\def\Si{\Sigma}
\def\th{\theta}
\def\z{\zeta}
\def\p{\partial}
\def\o{\omega}
\def\O{\Omega}
\def\t{\tau}
\def\L{\it \char'44}
\def\F{\mathcal{F}}
\def\B{\mathcal{B}}
\def\C{\mathcal{C}}
\def\half{\frac{1}{2}}
\ni m3f33prob4 \\
\begin{center}
{\bf M3F22 PROBLEMS 4.  3.11.2017} 
\end{center}
\ni{\it The Bivariate Normal Distribution}.  Define \hb
$$ f(x,y) = c \exp \{- {1 \over 2} Q(x,y) \},        $$
where $c$ is a constant, $Q$ a positive definite quadratic form in $x$
and $y$:
$$ c = {1 \over {2 \pi {\sigma}_1 {\sigma}_2 \sqrt{1 - {\rho}^2}}},  \qquad
 Q =
{1 \over {1 - {\rho}^2}}
\Bigl[{\Bigl({{x - {\mu}_1} \over
{{\sigma}_1}} \Bigr)}^2
- 2 \rho \Bigl({{x - {\mu}_1} \over
{{\sigma}_1}}\Bigr) \Bigl({{y - {\mu}_2} \over {{\sigma}_2}}\Bigr)
+
{\Bigl({{y - {\mu}_2} \over {{\sigma}_2}}\Bigr)}^2\Bigr].    $$
Here ${\sigma}_i > 0$, ${\mu}_i$ are real, $-1 < \rho < 1$.  Show that:\hb

\ni Q1.  $f$ is a probability density -- that is, that $f$ is non-negative and integrates
to 1. \hb

\ni Q2.  If $f$ is the density of a random 2-vector $(X,Y)$, $X$ and $Y$ are normal, with
distributions $N({\m}_1,{\s}_1^2)$, $N({\m}_2,{\s}_2^2)$. \hb

\ni Q3.  $X$, $Y$ have means ${\m}_1$, ${\m}_2$ and variances ${\s}_1^2$, ${\s}_2^2$. \hb

\ni Q4.  The conditional distribution of $y$ given $X = x$ is
$$
Y | (X = x) \sim N({\mu}_2 + \rho {{{\sigma}_2} \over {{\sigma}_1}} (x - {\mu}_1),\quad {\sigma}_2^2 (1 - {\rho}^2)).
$$

\ni Q5. The conditional mean $E(Y \vert X = x)$ is {\it
linear} in $x$:
$$ E(Y \vert X = x) = {\mu}_2 + \rho {{{\sigma}_2} \over
{{\sigma}_1}} (x - {\mu}_1).
$$
Q6. The conditional variance is $var[Y|X] = {\s}_2^2(1 - {\rho}^2)$. \hb
Q7. The correlation coefficient of $X$, $Y$ is $\rho$. \hb
Q8. The density $f$ has elliptical contours [i.e., the curves $f(x,y)$ constant are ellipses]. \hb
Q9. The joint MGF and joint CF of $X,Y$ are
$$M_{X,Y}(t_1,t_2) = M(t_1,t_2) = \exp({\mu}_1 t_1 + {\mu}_2 t_2 + {1
\over 2}[{\sigma}_1^2 t_1^2 + 2 \rho {\sigma}_1 {\sigma}_2 t_1 t_2
 + {\sigma}_2^2 t_2^2]),
$$
$${\phi}_{X,Y}(t_1,t_2) = {\phi}(t_1,t_2) = \exp(i {\m}_1 t_1 + i {\m}_2 t_2 - {1 \over 2}
[{\sigma}_1^2 t_1^2 + 2 \rho {\s}_1 {\s}_2 t_1 t_2 + {\s}_2 t_2^2]).
$$
Q10.  $X,Y$ are independent if and only if $\rho = 0$.\hb

\ni{\it Note}.  For those of you with a background in Statistics, this will be familiar
material.  It is included here as it serves as a very concrete illustration of the more abstract
conditioning of III.5,6 via the Radon-Nikodym Theorem.  For those of you without a background in
Statistics: the key here is {\it completing the
square} (the method you first encountered in learning how to solve quadratic equations).  If
you need help, find a good textbook on Statistics and look up `bivariate normal distribution'
in the index (it's in I.5 of [BF], Bingham and Fry, {\sl Regression}). \hfil NHB \break

\end{document}